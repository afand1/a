{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc9bf0c-9dc6-407b-82de-a26208e4d327",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#1.Explain the role of activation functions in neural networks. Compare and contrast linear and nonlinear activation functions. Why are nonlinear activation functions preferred in hidden layers.\n",
    "\"\"\"\n",
    "1.Explain the role of activation functions in neural networks. Compare and contrast linear and nonlinear activation functions. Why are nonlinear activation functions preferred in hidden layers.\n",
    "Ans:\n",
    "Activation functions in neural networks play a crucial role in determining the output of a neuron and, by extension, the overall behavior of the network. These functions introduce non-linearity into the network, enabling it to model complex data patterns and learn intricate relationships.\n",
    "\n",
    " Role of Activation Functions in Neural Networks\n",
    "1.Mapping Inputs to Outputs: Activation functions map the weighted sum of inputs from the previous layer to an output that can be passed to the next layer. This helps determine whether a neuron should be \"activated\" or not.\n",
    "   \n",
    "2. Introducing Non-Linearity: Activation functions allow the network to model non-linear relationships, which are critical for tasks like image recognition, language processing, and more. Without non-linearity, the neural network would essentially be a linear model, limiting its ability to solve complex problems.\n",
    "\n",
    "3. Gradient Flow in Backpropagation: During backpropagation, activation functions help determine how gradients flow through the network, which influences how effectively the network learns.\n",
    "\n",
    " Linear vs. Nonlinear Activation Functions\n",
    "1. Linear Activation Function\n",
    "   A linear activation function is defined as:\n",
    "   \\[  f(x) = ax\\]\n",
    "   where \\(a\\) is a constant. Here, the output is directly proportional to the input.\n",
    "\n",
    "   Characteristics:\n",
    "   - Simplicity: The output is a straightforward function of the input.\n",
    "   - No Non-linearity: The function does not introduce any non-linearity, meaning the output is always a linear transformation of the input.\n",
    "\n",
    "   Pros:\n",
    "   - Easy to compute and implement.\n",
    "   - Useful in the output layer for regression tasks.\n",
    "\n",
    "  Cons:\n",
    "   - Lack of complexity: A network with only linear activation functions, regardless of its depth, behaves like a single-layer model. It cannot capture complex patterns.\n",
    "   - Gradient issues: Since the derivative of a linear function is constant, gradients remain the same during backpropagation, limiting the model's learning capability.\n",
    "\n",
    " 2. Nonlinear Activation Functions\n",
    "   Nonlinear activation functions include functions like ReLU (Rectified Linear Unit), sigmoid, and tanh, which introduce non-linearity to the model.\n",
    "\n",
    "   - ReLU (Rectified Linear Unit): \n",
    "     \\[\n",
    "     f(x) = \\max(0, x)\n",
    "     \\]\n",
    "     It outputs \\(x\\) if \\(x > 0\\), otherwise it outputs 0. It is widely used in hidden layers.\n",
    "\n",
    "   - Sigmoid:\n",
    "     \\[\n",
    "     f(x) = \\frac{1}{1 + e^{-x}}\n",
    "     \\]\n",
    "     It squashes the output to be in the range of (0, 1), making it useful for binary classification tasks.\n",
    "\n",
    "   - Tanh (Hyperbolic Tangent):\n",
    "     \\[\n",
    "     f(x) = \\tanh(x)\n",
    "     \\]\n",
    "     It outputs values between -1 and 1, providing a smoother gradient than sigmoid.\n",
    "\n",
    "   Characteristics:\n",
    "   - Non-linearity: These functions allow the network to learn complex patterns by introducing non-linear transformations.\n",
    "   - Differentiability: Nonlinear activation functions are differentiable, which is important for gradient-based optimization.\n",
    "\n",
    "   Pros:\n",
    "   - They enable deep networks to model complex, non-linear relationships between inputs and outputs.\n",
    "   - Nonlinear functions like ReLU help mitigate the vanishing gradient problem, allowing networks to train faster and more effectively.\n",
    "\n",
    "   Cons:\n",
    "   - Functions like sigmoid and tanh can suffer from the vanishing gradient problem, where gradients become very small, making training slow for deep networks.\n",
    "\n",
    "### Why Nonlinear Activation Functions Are Preferred in Hidden Layers\n",
    "1. Modeling Complex Patterns: Neural networks with nonlinear activation functions can represent and learn from complex data patterns. Without non-linearity, no matter how many layers are added, the network would behave like a linear model, which is incapable of solving problems like image classification, speech recognition, etc.\n",
    "\n",
    "2. Hierarchical Feature Learning: Nonlinear activation functions allow the network to learn hierarchical features, with lower layers capturing basic patterns and higher layers capturing more abstract representations.\n",
    "\n",
    "3. Avoiding Linearity: If all activation functions were linear, no matter how deep the network, it could be reduced to a single linear layer. Nonlinear activation functions prevent this collapse and allow networks to achieve better performance on complex tasks.\n",
    "\n",
    "In summary, while linear activation functions are simple and have limited use (such as in the output layer of regression models), nonlinear activation functions are crucial in hidden layers because they enable neural networks to model the complex, non-linear relationships necessary for solving real-world problems.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19facd59-5daa-43c9-87ca-b994a7193b02",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2) Describe the Sigmoid activation function. What are its characteristics, and in what type of layers is it\n",
    "#commonly used? Explain the Rectified Linear Unit (ReLU) activation function. Discuss its advantages\n",
    "#and potential challenges.What is the purpose of the Tanh activation function? How does it differ from\n",
    "#the Sigmoid activation function.\n",
    "\"\"\"\n",
    "\n",
    "Ans:\n",
    "Sigmoid Activation Function\n",
    "The sigmoid activation function is one of the earliest and most commonly used activation functions in neural networks, defined as:\n",
    "f(x)=11+e−xf(x) = \\frac{1}{1 + e^{-x}}f(x)=1+e−x1\n",
    "Characteristics of Sigmoid\n",
    "1.\tRange: The sigmoid function outputs values between 000 and 111, making it suitable for binary classification tasks.\n",
    "2.\tS-shaped curve: The sigmoid curve is smooth and \"S\"-shaped, which allows for a gradual transition between outputs.\n",
    "3.\tNon-linearity: It is a non-linear function, which allows the network to learn complex patterns.\n",
    "4.\tOutput Interpretation: The output can be interpreted as a probability, which is why it's frequently used in the output layer for binary classification tasks.\n",
    "Common Use Cases\n",
    "•\tOutput Layer for Binary Classification: Sigmoid is often used in the output layer of neural networks when performing binary classification, as it converts the output to a value between 0 and 1, which can be interpreted as the probability of a class.\n",
    "•\tLogistic Regression: It is used in logistic regression models as the activation function to convert the linear combination of inputs into a probability.\n",
    "Challenges\n",
    "•\tVanishing Gradient Problem: In deep networks, the gradient of the sigmoid function becomes very small (close to zero) for large positive or negative inputs. This leads to very slow learning, especially in deeper layers.\n",
    "•\tNon-zero-centered Output: The output of the sigmoid function is always positive, which can result in inefficiencies during backpropagation, as gradients may accumulate in only one direction.\n",
    "________________________________________\n",
    "Rectified Linear Unit (ReLU) Activation Function\n",
    "The ReLU activation function is one of the most popular activation functions in modern neural networks, especially in deep learning. It is defined as:\n",
    "f(x)=max⁡(0,x)f(x) = \\max(0, x)f(x)=max(0,x)\n",
    "Characteristics of ReLU\n",
    "1.\tRange: ReLU outputs values between 000 and ∞\\infty∞. For any input x>0x > 0x>0, ReLU returns xxx, and for x≤0x \\leq 0x≤0, it returns 0.\n",
    "2.\tNon-linearity: Despite its simple form, ReLU introduces non-linearity, which allows networks to learn complex data representations.\n",
    "3.\tSparse Activation: Since ReLU outputs zero for all negative inputs, it leads to sparse activation (many neurons will not activate), making the network more efficient by reducing computation.\n",
    "Advantages of ReLU\n",
    "•\tEfficient Computation: ReLU is computationally simple, as it requires only a thresholding operation, making it highly efficient for large-scale deep networks.\n",
    "•\tAlleviates Vanishing Gradient Problem: Unlike sigmoid and tanh, ReLU does not saturate for positive inputs, which helps prevent the vanishing gradient problem and enables faster training in deep networks.\n",
    "•\tPromotes Sparse Representations: By outputting zero for negative values, ReLU leads to sparsity in the network, which can improve generalization and efficiency.\n",
    "Challenges of ReLU\n",
    "•\tDying ReLU Problem: For some neurons, if the input to ReLU is always negative during training, they will output zero and stop learning entirely. This is known as the \"dying ReLU\" problem, which can prevent the network from learning efficiently.\n",
    "•\tUnbounded Output: The ReLU function does not have an upper bound, which may result in very large activations that could destabilize the network if not handled with proper weight initialization or regularization techniques.\n",
    "________________________________________\n",
    "Tanh (Hyperbolic Tangent) Activation Function\n",
    "The tanh activation function is another popular activation function, defined as:\n",
    "f(x)=tanh⁡(x)=ex−e−xex+e−xf(x) = \\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}f(x)=tanh(x)=ex+e−xex−e−x\n",
    "Characteristics of Tanh\n",
    "1.\tRange: The tanh function outputs values between −1-1−1 and 111, which means its output is zero-centered.\n",
    "2.\tS-shaped curve: Like the sigmoid function, tanh has an \"S\"-shaped curve, but it is steeper.\n",
    "3.\tNon-linearity: It introduces non-linearity and allows the network to model complex relationships.\n",
    "Purpose and Use Cases\n",
    "•\tHidden Layers in Neural Networks: Tanh is commonly used in the hidden layers of neural networks. Its zero-centered nature can make it a better choice than sigmoid for layers where the data needs to have both positive and negative outputs.\n",
    "•\tNormalization: Since the tanh function outputs both positive and negative values, it helps to normalize the input data to a mean of zero, which can improve convergence speed in training.\n",
    "________________________________________\n",
    "Differences Between Tanh and Sigmoid\n",
    "1.\tRange:\n",
    "o\tSigmoid: Outputs values between 000 and 111, making it suitable for binary classification.\n",
    "o\tTanh: Outputs values between −1-1−1 and 111, which is more useful for modeling both positive and negative values.\n",
    "2.\tZero-centered Output:\n",
    "o\tSigmoid: Not zero-centered, which means it can introduce bias during backpropagation since gradients may be always positive.\n",
    "o\tTanh: Zero-centered, which allows gradients to flow in both positive and negative directions, improving the learning process.\n",
    "3.\tSteepness:\n",
    "o\tTanh: Tanh is steeper than sigmoid, making it more sensitive to changes in input values.\n",
    "o\tSigmoid: Flatter in the outer regions, leading to saturation for very large or very small input values, which can cause vanishing gradients.\n",
    "In summary, sigmoid is commonly used in output layers for binary classification, ReLU is widely adopted in hidden layers due to its simplicity and efficiency in deep learning, and tanh is often preferred over sigmoid for hidden layers where zero-centered outputs are beneficial.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5b910e-1ebd-4413-96c7-6d8384d2e8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3. Discuss the significance of activation functions in the hidden layers of a neural network-\n",
    "\"\"\"\n",
    "Ans:\n",
    "Activation functions in the hidden layers of a neural network are crucial for the network's ability to learn and model complex, non-linear relationships in data. Without these activation functions, the network would behave like a linear regression model, even if it had multiple layers, rendering it incapable of solving intricate problems like image recognition, language processing, or classification tasks.\n",
    "Here’s a detailed discussion on the significance of activation functions in the hidden layers:\n",
    "1. Introducing Non-Linearity\n",
    "•\tNon-linear Mapping: Activation functions in the hidden layers introduce non-linearity to the model, enabling the network to map non-linear relationships between inputs and outputs. Real-world problems often involve data that cannot be separated or understood using linear transformations alone. Non-linear activation functions allow neural networks to capture patterns in data that are complex and multi-dimensional.\n",
    "•\tHierarchical Feature Learning: Non-linear activation functions enable the network to learn hierarchical representations of data. Lower layers can capture simple features (e.g., edges in an image), while higher layers learn more abstract representations (e.g., faces or objects). Without non-linearity, each layer would only perform linear transformations, and the network would struggle to learn these hierarchical features.\n",
    "2. Enabling Deep Learning\n",
    "•\tDeeper Networks: Activation functions allow neural networks to stack multiple layers (deep networks) while still performing meaningful computations. Each layer, equipped with non-linear activation, helps transform the data in a way that the next layer can build upon, which is crucial for the success of deep learning architectures.\n",
    "•\tIncreased Learning Capacity: Non-linear activations increase the learning capacity of the network, allowing it to approximate highly complex functions. This allows neural networks to perform tasks like image classification, language translation, and game playing, where simple linear models would fail.\n",
    "3. Handling Complex Data Patterns\n",
    "•\tFeature Interactions: Hidden layers with non-linear activations are capable of capturing interactions between input features. For instance, in a neural network for image classification, non-linear activations enable the network to understand that certain combinations of pixels (features) together might represent a more meaningful object (like a corner or edge) than individual pixels alone.\n",
    "•\tLearning Complex Representations: With non-linear activation functions, the hidden layers transform the input data into a more abstract and useful representation. This allows the network to break down complex tasks into simpler, smaller problems that can be solved at each layer.\n",
    "4. Improving Gradient Flow in Backpropagation\n",
    "•\tGradient Descent: In backpropagation, the activation functions determine the gradients that are propagated back through the network. Without non-linear activations, the gradients would not properly adjust the weights in a way that improves performance. Non-linear activations like ReLU help avoid issues like vanishing gradients and allow efficient learning even in deep networks.\n",
    "•\tAvoiding the Vanishing Gradient Problem: Certain non-linear activations like ReLU help reduce the vanishing gradient problem, which is a common issue in deep networks with activation functions like sigmoid or tanh. By outputting zero for negative values and the input for positive values, ReLU allows gradients to flow more freely through the network, especially in deep layers.\n",
    "5. Flexibility in Architecture\n",
    "•\tChoice of Activation Function: Neural networks benefit from different activation functions depending on the problem and the specific layer. For instance, in hidden layers, ReLU or Leaky ReLU is often preferred due to its simplicity and efficiency, while in output layers, sigmoid or softmax is used for tasks like binary and multi-class classification. The ability to use different activations in different parts of the network adds flexibility to the design and improves performance on a variety of tasks.\n",
    "•\tDifferent Layers, Different Roles: Hidden layers perform complex transformations on data, and their activation functions must balance non-linearity with gradient flow. Choosing the right activation function ensures that each hidden layer contributes to the learning process and that the network remains trainable.\n",
    "6. Learning Features with Zero-Centered Outputs\n",
    "•\tZero-Centered Activations: Functions like tanh have outputs that range between -1 and 1, providing zero-centered outputs. This is beneficial for training because it ensures that the output of hidden layers is not biased towards positive or negative values. This improves the learning dynamics by ensuring gradients flow symmetrically during backpropagation, leading to faster convergence.\n",
    "•\tNormalization of Data: Some non-linear activation functions can help normalize the outputs of each layer. For instance, using Batch Normalization in conjunction with ReLU can help stabilize learning by ensuring that each hidden layer maintains a consistent distribution of values.\n",
    "7. Regularization and Generalization\n",
    "•\tSparse Activation: Functions like ReLU introduce sparsity by deactivating neurons with negative inputs (outputting zero). Sparse representations can improve generalization and reduce the risk of overfitting by ensuring that only a subset of neurons are active at any given time. This acts as a form of regularization.\n",
    "•\tPreventing Overfitting: By introducing non-linearity, the network is less likely to memorize the training data (which can lead to overfitting) and is more likely to generalize well to new, unseen data.\n",
    "Conclusion\n",
    "Activation functions in the hidden layers are essential for allowing neural networks to model complex, non-linear patterns, capture hierarchical feature representations, and enable deep learning. They play a critical role in ensuring that gradients flow effectively during backpropagation, help networks handle complex data, and allow for flexibility in architecture design. Non-linear activation functions, like ReLU, tanh, and others, ensure that the hidden layers of a network contribute to learning in meaningful ways, transforming the input data into representations that ultimately lead to better predictions and performance.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4877335c-cce3-482d-845a-dd99ab5af59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#4 .Explain the choice of activation functions for different types of problems (e.g., classification, regression) in the output layer-\n",
    "\"\"\"\n",
    "The choice of activation function in the output layer of a neural network depends on the type of problem being solved, such as classification, regression, or multi-class classification. The activation function must ensure that the output format matches the requirements of the problem, and it must facilitate effective learning during training.\n",
    "1. Classification Problems\n",
    "For classification tasks, the goal is to predict the class or category of a given input. The output layer typically provides a probability or score that can be interpreted as a classification.\n",
    "A. Binary Classification\n",
    "In binary classification problems, where there are only two possible classes (e.g., yes/no, spam/not spam), the most common activation function used in the output layer is the sigmoid function.\n",
    "•\tSigmoid Activation Function:\n",
    "f(x)=11+e−xf(x) = \\frac{1}{1 + e^{-x}}f(x)=1+e−x1\n",
    "Why Sigmoid?\n",
    "o\tRange: The sigmoid function outputs values between 000 and 111, making it ideal for binary classification, where the output represents the probability of one of the classes (e.g., probability of being class 1).\n",
    "o\tOutput Interpretation: The output of the sigmoid can be interpreted as the likelihood that a given input belongs to the positive class. A threshold, often 0.50.50.5, is used to determine class labels.\n",
    "Example: Email spam detection (spam vs. not spam).\n",
    "B. Multi-Class Classification\n",
    "For multi-class classification, where the task is to classify an input into one of several categories (e.g., digit recognition), the softmax function is commonly used in the output layer.\n",
    "•\tSoftmax Activation Function:\n",
    "f(xi)=exi∑j=1nexjf(x_i) = \\frac{e^{x_i}}{\\sum_{j=1}^{n} e^{x_j}}f(xi)=∑j=1nexjexi\n",
    "Why Softmax?\n",
    "o\tProbabilistic Output: Softmax outputs a probability distribution over all possible classes. Each output neuron corresponds to a class, and the softmax ensures that the sum of all probabilities across classes is 1.\n",
    "o\tMulti-Class Classification: It is designed for multi-class classification problems where an input needs to be assigned to one of several mutually exclusive categories.\n",
    "Example: Handwritten digit recognition (digits 0-9).\n",
    "C. Multi-Label Classification\n",
    "In multi-label classification, where each input can belong to multiple classes (e.g., image tagging where an image can have multiple labels like \"cat,\" \"dog,\" etc.), the sigmoid function is applied to each output neuron individually.\n",
    "•\tWhy Sigmoid?\n",
    "o\tFor each label, the sigmoid function outputs a probability between 0 and 1, and thresholds (like 0.5) are used to decide whether each label is assigned.\n",
    "Example: Image tagging (an image could be tagged with multiple categories like \"beach,\" \"sunset,\" and \"people\").\n",
    "________________________________________\n",
    "2. Regression Problems\n",
    "For regression tasks, where the goal is to predict a continuous value (e.g., price, temperature, or age), the activation function in the output layer must provide unbounded values that can represent real-world quantities.\n",
    "A. Linear Activation Function (No Activation)\n",
    "In many regression problems, the output layer uses a linear activation function, which essentially means no activation function is applied:\n",
    "f(x)=xf(x) = xf(x)=x\n",
    "Why Linear?\n",
    "•\tUnbounded Output: The linear activation function allows the output to take any real value, which is necessary for regression problems where the target variable can range over the entire real number line.\n",
    "•\tDirect Mapping: The output is a direct linear combination of the inputs and weights, which fits well with the goals of regression tasks.\n",
    "Example: Predicting house prices, stock market predictions.\n",
    "B. Other Considerations for Regression\n",
    "•\tIf the output range is constrained (e.g., predicting percentages between 0 and 100), a sigmoid function or a scaled version of it could be used.\n",
    "•\tSimilarly, if the output needs to be in a specific range like −1-1−1 to 111, a tanh activation function may be used.\n",
    "________________________________________\n",
    "3. Other Types of Problems\n",
    "A. Ordinal Regression\n",
    "In ordinal regression, the task is to predict a rank or ordered category, such as predicting customer satisfaction on a scale of 1 to 5. The activation function here often depends on the way the problem is framed:\n",
    "•\tIf framed as a multi-class classification problem, softmax might be used.\n",
    "•\tIf framed as a regression problem predicting a continuous value that is later rounded, a linear activation might be used.\n",
    "________________________________________\n",
    "Summary of Activation Functions for Different Problems\n",
    "Problem Type\tOutput Activation Function\tReason\n",
    "Binary Classification\tSigmoid\tOutputs probability for binary outcomes.\n",
    "Multi-Class Classification\tSoftmax\tOutputs a probability distribution for multiple classes.\n",
    "Multi-Label Classification\tSigmoid\tOutputs independent probabilities for each label.\n",
    "Regression (Unbounded)\tLinear (None)\tAllows unbounded continuous output values.\n",
    "Regression (Bounded)\tSigmoid or Tanh\tRestricts output within a specific range.\n",
    "The activation function used in the output layer is selected based on the problem's requirements, ensuring that the model's outputs are suitable for the task at hand, whether it's predicting probabilities, class labels, or continuous values.\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74ff76ec-8bc1-4398-9a8f-6d5f53f74d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5.Experiment with different activation functions (e.g., ReLU, Sigmoid, Tanh) in a simple neural network architecture. Compare their effects on convergence and performance\n",
    "\"\"\"\n",
    "\n",
    "Ans:\n",
    "To experiment with different activation functions (ReLU, Sigmoid, Tanh) in a simple neural network and compare their effects on convergence and performance, you can follow these steps:\n",
    "1. Create a Simple Neural Network\n",
    "We will use a simple feed-forward neural network to demonstrate the impact of different activation functions. The network will have:\n",
    "•\tInput layer: Accepting features from a dataset.\n",
    "•\tHidden layer(s): Applying different activation functions (ReLU, Sigmoid, Tanh).\n",
    "•\tOutput layer: Appropriate activation based on the problem type (e.g., Sigmoid for binary classification).\n",
    "You can use a common framework like TensorFlow/Keras or PyTorch for implementation. Below, I'll outline the process using Keras (TensorFlow backend).\n",
    "2. Dataset\n",
    "We will use the MNIST dataset, which consists of handwritten digits (0-9) and is often used for classification tasks.\n",
    "3. Implementation Steps\n",
    "•\tSet up the neural network using Keras.\n",
    "•\tExperiment with different activation functions (ReLU, Sigmoid, Tanh) in the hidden layers.\n",
    "•\tCompare the performance and convergence during training.\n",
    "\n",
    "Code:\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare the MNIST dataset\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0  # Normalize the pixel values\n",
    "\n",
    "# Convert labels to categorical format (One-hot encoding)\n",
    "y_train = tf.keras.utils.to_categorical(y_train, 10)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, 10)\n",
    "\n",
    "# Function to build a simple feed-forward neural network\n",
    "def build_model(activation_function):\n",
    "    model = Sequential()\n",
    "    model.add(Flatten(input_shape=(28, 28)))  # Flatten input images (28x28)\n",
    "    model.add(Dense(128, activation=activation_function))  # Hidden layer\n",
    "    model.add(Dense(10, activation='softmax'))  # Output layer (Softmax for multi-class classification)\n",
    "    model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "# Train and evaluate the model with different activation functions\n",
    "activations = ['relu', 'sigmoid', 'tanh']\n",
    "history = {}\n",
    "\n",
    "for activation in activations:\n",
    "    print(f\"Training with {activation} activation function...\")\n",
    "    model = build_model(activation)\n",
    "    history[activation] = model.fit(x_train, y_train, epochs=10, batch_size=64, validation_data=(x_test, y_test), verbose=1)\n",
    "\n",
    "# Plot accuracy comparison for each activation function\n",
    "plt.figure(figsize=(10, 6))\n",
    "for activation in activations:\n",
    "    plt.plot(history[activation].history['val_accuracy'], label=f'{activation} activation')\n",
    "\n",
    "plt.title('Comparison of Validation Accuracy with Different Activation Functions')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Validation Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "5. Explanation of the Code\n",
    "•\tData Preparation: The MNIST dataset is loaded, and the pixel values are normalized between 0 and 1. Labels are one-hot encoded for multi-class classification.\n",
    "•\tModel Structure:\n",
    "o\tThe network uses a Flatten layer to transform the 2D images into 1D vectors.\n",
    "o\tOne hidden layer is included, with the activation function being set to either ReLU, Sigmoid, or Tanh.\n",
    "o\tThe output layer uses the Softmax function to output class probabilities.\n",
    "•\tTraining: The model is compiled with the Adam optimizer and trained for 10 epochs on the training set, with validation on the test set.\n",
    "•\tComparison: After training with different activation functions, the validation accuracy is plotted to compare the performance.\n",
    "6. Expected Results and Comparison\n",
    "You can expect to see differences in both convergence speed and overall performance (validation accuracy). Here’s a general idea of what to expect:\n",
    "A. ReLU (Rectified Linear Unit)\n",
    "•\tPerformance: ReLU is likely to converge faster due to its efficient gradient propagation, especially in deeper networks.\n",
    "•\tEffectiveness: It generally works well for most deep networks, with faster training and high accuracy.\n",
    "•\tChallenges: May suffer from the \"dying ReLU\" problem, where neurons can stop updating their weights if they only output zeros for all inputs.\n",
    "B. Sigmoid\n",
    "•\tPerformance: Training with sigmoid can be slow because of the vanishing gradient problem, especially in deeper networks. The gradients become very small for large positive or negative inputs, slowing down learning.\n",
    "•\tEffectiveness: Sigmoid is often not as effective as ReLU in deep networks, though it can work well for binary classification problems with shallow networks.\n",
    "•\tChallenges: Gradients can vanish, leading to slower convergence.\n",
    "C. Tanh\n",
    "•\tPerformance: Tanh also suffers from the vanishing gradient problem but can sometimes outperform sigmoid because it is zero-centered, which helps gradients flow better.\n",
    "•\tEffectiveness: Tanh can be useful in certain shallow networks but generally performs worse than ReLU in deeper architectures.\n",
    "•\tChallenges: Similar to sigmoid, it may lead to slow learning in deeper networks.\n",
    "7. Conclusion\n",
    "•\tReLU generally performs the best in terms of convergence speed and accuracy, especially in deep networks. It is widely used for hidden layers in modern neural networks.\n",
    "•\tSigmoid and Tanh tend to be slower due to the vanishing gradient problem, but they can still be useful in specific tasks, such as binary classification (sigmoid) or when the data needs to be normalized between -1 and 1 (tanh).\n",
    "•\tVisualization: The plotted validation accuracy should clearly show how ReLU tends to converge faster than sigmoid and tanh, with higher final accuracy.\n",
    "Feel free to run this code and observe the performance differences between these activation functions in terms of accuracy and convergence rate.\n",
    "\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de215b21-8641-42da-a843-f1ed4c421458",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
